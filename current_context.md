# Project Context Handoff

## What We're Building

A multi-tenant SaaS platform that uses AI to generate social media threads and articles from user-submitted topics. It performs research, summarisation, and content creation, and can publish to X (Twitter) or Typefully.

---

## Current Phase

**Phase 2: Basic Pipeline – Summary Agent** ✅ COMPLETE

---

## Tech Stack

* Python 3.11+ / FastAPI
* PostgreSQL + SQLAlchemy ORM
* Alembic for migrations
* Celery + Redis for async tasks
* OpenAI / Anthropic API support
* Docker for local and cloud deployment

---

## AI Model Configuration Strategy

We support model overrides on a **per-agent basis**, with fallback to a global default. This is handled in `llm/engine.py` using a `DEFAULT_MODELS` dictionary and `settings` from `config.py`.

### `.env` Example:

```dotenv
OPENAI_MODEL=gpt-4
OPENAI_MODEL_TOPIC_AGENT=gpt-4
OPENAI_MODEL_SUMMARY_AGENT=gpt-4-turbo
OPENAI_MODEL_CONTENT_AGENT=gpt-4-turbo
OPENAI_MODEL_RESEARCH_AGENT=gpt-4-turbo
```

### In `config.py`:

```python
class Settings(BaseSettings):
    ...
    openai_model: str = "gpt-4"
    openai_model_topic_agent: str = "gpt-4"
    openai_model_summary_agent: str = "gpt-4-turbo"
    openai_model_content_agent: str = "gpt-4-turbo"
    openai_model_research_agent: str = "gpt-4-turbo"
```

---

## File/Folder Structure

```
xPostingAgent-as-a-Service/
├── alembic/                     # Migrations
│   ├── versions/               # Autogenerated migration files
│   └── env.py                  # Alembic config
├── app/
│   ├── __init__.py
│   ├── main.py                 # FastAPI app entrypoint
│   ├── config.py               # Loads settings from .env
│   ├── database.py             # DB engine, session factory
│   ├── models/
│   │   ├── __init__.py
│   │   ├── users.py            # User table
│   │   ├── requests.py         # Request table
│   │   ├── research_sources.py # Source metadata per request
│   │   └── summaries.py        # ✅ NEW: Stores combined summary + key points
│   ├── agents/
│   │   ├── research_agent.py   # Research Agent: suggests + verifies sources
│   │   └── summary_agent.py    # ✅ NEW: Summarises research into summary + key points
│   ├── prompts/
│   │   ├── research_prompt.py  # Prompt builder for research agent
│   │   └── summary_prompt.py   # ✅ NEW: Prompt builder for summary agent
│   ├── llm/
│   │   ├── __init__.py
│   │   └── engine.py           # Handles GPT calls with agent-aware model lookup
├── insert_dummy_data.py        # Script to insert test user/request
├── test_run_research_agent.py  # Manual test for the research agent
├── app/tests/
│   └── test_run_summary_agent.py # ✅ NEW: Manual test for the summary agent
├── .env
├── requirements.txt
```

---

## Defined Classes and Functions

### `app/config.py`

* `Settings(BaseSettings)`: Loads environment variables for model config and runtime

  * Includes model overrides per agent (e.g. `openai_model_summary_agent`)

### `app/database.py`

* `engine`: SQLAlchemy DB engine
* `SessionLocal`: Session maker for transactions
* `Base`: SQLAlchemy declarative base

### `app/models/`

#### `research_sources.py`

* `ResearchSource`: SQLAlchemy model for storing:

  * `url`, `title`, `author`, `verification_status`, `relevance_score`, `freshness_score`, `last_verified_at`, `summary`, `key_points`, `is_used`, etc.

#### `summaries.py`

* `Summary`: SQLAlchemy model for storing final summarisation output:

  * `request_id`, `user_id`, `combined_summary`, `combined_key_points`, `source_count`, `is_used`, `created_at`

### `app/llm/engine.py`

* `generate_completion(prompt, model_name=None, agent="default")`

  * Selects model using `DEFAULT_MODELS.get(agent, settings.openai_model)`
  * Prints active model for debugging

### `app/prompts/summary_prompt.py`

* `build_summary_prompt(text, key_points, length_limit, content_type)`:

  * Builds a structured LLM prompt for summarisation using verified source inputs

### `app/agents/summary_agent.py`

* `combine_summaries(sources)`:

  * Combines all source summaries into one text block

* `merge_key_points(sources)`:

  * Deduplicates and merges key points across all verified sources

* `parse_llm_output(response_text)`:

  * Splits LLM response into summary and key points section

* `generate_and_store_summary(request_id, verified_sources, target_length, content_type, user_id)`:

  * Main pipeline that:

    1. Combines summaries
    2. Builds prompt
    3. Calls LLM
    4. Parses result
    5. Saves to `summaries` table

### `insert_dummy_data.py`

* Inserts dummy `User` and `Request` into DB for testing

### `test_run_research_agent.py`

* Manual test runner for `generate_research_sources()` pipeline

### `test_run_summary_agent.py`

* ✅ NEW manual test runner for summary agent
* Inserts dummy request + verified sources with `summary`/`key_points`
* Runs `generate_and_store_summary()` and prints results

---

## Tests

* Manual test scripts:

  * `test_run_research_agent.py`: prints verified research sources
  * `test_run_summary_agent.py`: prints summary + key points generated from sources

---

## What’s Next

🕸️ In the next phase, we will:

1. Implement the **Content Generation Agent**
2. Take summaries + key points → turn into engaging threads/articles
3. Store output in `content_queue` table for review/posting

---

*Last updated: 2025-06-19*
